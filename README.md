# Test GPT-2 and CodeBERT

This repository contains code and scripts to experiment with fine-tuning GPT-2 and CodeBERT models, as well as automating the training process and evaluating the model's responses.

## Contents

- `trainer_gpt2.py`: Script to fine-tune the GPT-2 model and save the fine-tuned model.
- `test_finetuned_gpt2.py`: Script to test the fine-tuned GPT-2 model on given prompts.
- `trainer_codebert.py`: Script to fine-tune the CodeBERT model and save the fine-tuned model.
- `test_finetuned_codebert.py`: Script to test the fine-tuned CodeBERT model on code-related prompts.
- `optimizer.py`: Script to automate parameter tuning using an optimizer based on response evaluations.

## Getting Started

1. Install the required libraries and dependencies.
2. Fine-tune the models using the training scripts.
3. Test the fine-tuned models using the testing scripts.
4. Use the optimizer script to automate parameter tuning.

## Usage

1. Clone this repository.
2. Navigate to the directory of the desired script.
3. Run the script using Python: `python script_name.py`

## Contributions

Contributions to this project are welcome! If you find any issues or have suggestions for improvements, please feel free to create an issue or submit a pull request.

## License

This project is licensed under the [MIT License](LICENSE).


